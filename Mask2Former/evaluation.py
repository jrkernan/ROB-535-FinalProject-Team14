# -*- coding: utf-8 -*-
"""SS_BDD_Model_Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/120mjVKC8OHlZVmxodSPA80ax6YGpTmku
"""

from pyexpat import model
import importlib, types, sys

from tqdm import tqdm
from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
from PIL import Image
import requests
import torch
import numpy as np
import os
from pathlib import Path
import cv2
import numpy as np

from pathlib import Path
import os
import numpy as np
from PIL import Image
import torch
import cv2
from tqdm import tqdm

from matplotlib import pyplot as plt
from PIL import Image


ROOT_PATH = "/content/drive/My Drive/2 - 5th Year/self-driving/Self-Driving Project"

IMAGES_PATH = Path(os.path.join(ROOT_PATH, "datasets/BDD100k/images/val"))
MASKS_PATH = Path(os.path.join(ROOT_PATH, "datasets/BDD100k/drivable_area_annotations/val"))

EVAL_IMG_PATH = Path("/content/drive/My Drive/2 - 5th Year/self-driving/Self-Driving Project/Test Images")
PRED_SAVE_DIR = EVAL_IMG_PATH / "mask2former_overlays"

"""Evaluation Criteria:

  - **Intersection over Union (IoU)/Mean IoU (mIoU):** Overlap between the predicted road region and the ground truth
  - **Precision:** The proportion of correctly predicted road pixels among all pixels the model predicted as road
  - **Recall:** The proportion of actual road pixels that were correctly identified by the model
  - **F1 Score:** The harmonic mean of precision and recall, a single balanced measure of the model's accuracy.
"""



# ----------------------------
# METRIC ACCUMULATORS
# ----------------------------
def eval_pipeline():

    # HUGGING FACE USAGE: https://huggingface.co/docs/transformers/main/en/model_doc/mask2former
    # HUGGING FACE pretrained models: https://huggingface.co/models?search=mask2former
    pretrained_model = "facebook/mask2former-swin-large-cityscapes-semantic" # ex: facebook/mask2former-swin-small-ade-semantic

    SPLIT = "val"

    PRED_DRIVABLE_LABEL_NAMES = {"road"}


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    image_processor = AutoImageProcessor.from_pretrained(pretrained_model)
    model = Mask2FormerForUniversalSegmentation.from_pretrained(pretrained_model)
    model.to(device)
    model.eval()

    # We'll treat drivable segmentation as BINARY:
    id2label = model.config.id2label  # e.g. {0: 'wall', 1:'building', ...}
    num_labels = len(id2label)

    is_pred_road = np.zeros(num_labels, dtype=bool)
    for idx, name in id2label.items():
        normalized = name.lower().replace("_", " ").strip()
        if normalized in PRED_DRIVABLE_LABEL_NAMES:
            is_pred_road[idx] = True

    # print("Model label mapping (ROAD classes marked):")
    # for idx, name in id2label.items():
    #     flag = "ROAD" if is_pred_road[idx] else ""
    #     print(f"  {idx:2d}: {name} {flag}")

    tp = 0  # pred=1, gt=1
    fp = 0  # pred=1, gt=0
    fn = 0  # pred=0, gt=1
    tn = 0  # pred=0, gt=0

    BDD_ROAD_CLASS_ID = 0

    image_mask_pairs = []

    mask_files = sorted(MASKS_PATH.glob("*.png"))
    print("Num mask files found:", len(mask_files))

    for m_path in mask_files:
        # m_path.stem is e.g. "fe194677-e2d2ac8c_train_id"
        # remove suffix to match image stem
        mask_stem = m_path.stem
        img_stem = mask_stem.replace("_train_id", "")  # remove suffix
        img_path = IMAGES_PATH / (img_stem + ".jpg")

        img_path = IMAGES_PATH / (img_stem + ".jpg")
        if img_path.exists():
            image_mask_pairs.append((img_path, m_path))
        else:
            print(f"[WARN] No image for mask {m_path.name}, expected {img_path.name}")

    print("Num (image, mask) pairs to evaluate:", len(image_mask_pairs))


    for img_path, m_path in tqdm(image_mask_pairs, desc="Evaluating", unit="img"):
        if not img_path.exists():
            print(f"[WARN] No image for mask {m_path.name}, expected {img_path.name}")
            # print(f"[WARN] Missing mask for {img_path.name}, expected {m_path.name}")
            continue

        # Load image + GT mask
        image = Image.open(img_path).convert("RGB")
        gt_driv_mask = np.array(Image.open(m_path).convert("L"), dtype=np.uint8)

        # Run Mask2Former
        inputs = image_processor(images=image, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model(**inputs)

        h, w = image.height, image.width
        semseg = image_processor.post_process_semantic_segmentation(
            outputs, target_sizes=[(h, w)]
        )[0]  # tensor HxW with label IDs
        semseg = semseg.cpu().numpy()

        # semseg: predicted labels, gt_mask: ground-truth labels
        # Make sure shapes align (H, W)
        if semseg.shape != gt_driv_mask.shape:
            # Case 1: exactly transposed (e.g. (1280, 720) vs (720, 1280))
            if semseg.shape[::-1] == gt_driv_mask.shape:
                semseg = semseg.T  # transpose to match H x W
            else:
                # Fallback: resize semseg to GT mask shape (rare case)
                import cv2
                H, W = gt_driv_mask.shape
                semseg = cv2.resize(
                    semseg.astype(np.uint8),
                    (W, H),
                    interpolation=cv2.INTER_NEAREST,
                )

        # Prediction: road vs non-road
        pred_road = is_pred_road[semseg]  # bool mask
        image_np = np.array(image)  # H x W x 3, RGB
        pred_drivable = refine_drivable_keep_right_of_centerline_debug(image_np,pred_road)

        gt_drivable = (gt_driv_mask > 0)
        valid = np.ones_like(gt_drivable, dtype=bool)

        pred_flat = pred_drivable[valid].reshape(-1)
        gt_flat   = gt_drivable[valid].reshape(-1)

        tp += np.logical_and(pred_flat == 1, gt_flat == 1).sum()
        fp += np.logical_and(pred_flat == 1, gt_flat == 0).sum()
        fn += np.logical_and(pred_flat == 0, gt_flat == 1).sum()
        tn += np.logical_and(pred_flat == 0, gt_flat == 0).sum()

    # ----------------------------
    # COMPUTE METRICS
    # ----------------------------
    eps = 1e-6

    precision = tp / (tp + fp + eps)
    recall = tp / (tp + fn + eps)
    f1 = 2 * precision * recall / (precision + recall + eps)

    iou_road = tp / (tp + fp + fn + eps)
    iou_nonroad = tn / (tn + fp + fn + eps)
    miou_binary = 0.5 * (iou_road + iou_nonroad)

    pixel_acc = (tp + tn) / (tp + tn + fp + fn + eps)

    print("\n=== BDD100K Road Segmentation Evaluation (Mask2Former) ===")
    print(f"  Precision (road):     {precision:.4f}")
    print(f"  Recall (road):        {recall:.4f}")
    print(f"  F1 score (road):      {f1:.4f}")
    print(f"  IoU (road):           {iou_road:.4f}")
    print(f"  IoU (non-road):       {iou_nonroad:.4f}")
    print(f"  Binary mIoU:          {miou_binary:.4f}")
    print(f"  Pixel accuracy:       {pixel_acc:.4f}")
    print(f"  TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")


def refine_drivable_keep_right_of_centerline_debug(
    image_rgb: np.ndarray,
    road_mask: np.ndarray,
    crop_ratio: float = 0.4,
    min_yellow_pixels_row: int = 10,
    min_total_yellow: int = 300,
    gaussian_smooth_rows: int = 9,
    force_cut_if_no_yellow: bool = False,
    fallback_offset_ratio: float = 0.05,
) -> np.ndarray:
    """
    Use yellow-lane detection to build a *curve* x(y) for the divider,
    then keep only road pixels to the RIGHT of that curve.

    image_rgb: HxWx3 RGB image.
    road_mask: HxW bool or {0,1} road prediction.
    """

    h, w = road_mask.shape
    road_bool = road_mask.astype(bool)

    # --- 1) HSV + bottom ROI ---
    hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)
    top = int(h * crop_ratio)
    hsv[:top, :] = 0  # ignore sky / upper area

    # Yellow range (tune if needed)
    lower_yellow = np.array([10,  40,  80], dtype=np.uint8)
    upper_yellow = np.array([45, 255, 255], dtype=np.uint8)

    yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)
    yellow_mask[~road_bool] = 0  # only consider yellow on predicted road

    # Morphological clean-up
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    yellow_mask = cv2.morphologyEx(yellow_mask, cv2.MORPH_CLOSE, kernel, iterations=2)

    total_yellow = int(np.count_nonzero(yellow_mask))
    print("Total yellow pixels in ROI:", total_yellow)

    if total_yellow < min_total_yellow:
        # Not enough yellow; optionally fall back to simple center-ish cut
        if not force_cut_if_no_yellow:
            print("Too little yellow; returning original road mask.")
            return road_bool

        center_x = w // 2
        divider_x = int(center_x - fallback_offset_ratio * w)
        print(f"Too little yellow; fallback vertical cut at x = {divider_x}")
        drivable = road_bool.copy()
        before = np.sum(drivable)
        drivable[:, :divider_x] = False
        after = np.sum(drivable)
        print("Fallback removed pixels:", before - after)
        return drivable

    # --- 2) Per-row yellow center (x(y)) ---
    row_indices = []
    row_centers = []

    for y in range(top, h):
        cols = np.where(yellow_mask[y, :] > 0)[0]
        if cols.size < min_yellow_pixels_row:
            continue
        # median is robust to outliers
        x_med = int(np.median(cols))
        row_indices.append(y)
        row_centers.append(x_med)

    if len(row_indices) == 0:
        # Shouldn't happen if total_yellow checked, but be safe
        print("No rows with enough yellow; using fallback.")
        center_x = w // 2
        divider_x = int(center_x - fallback_offset_ratio * w)
        drivable = road_bool.copy()
        drivable[:, :divider_x] = False
        return drivable

    row_indices = np.array(row_indices)
    row_centers = np.array(row_centers, dtype=np.float32)

    # --- 3) Interpolate + smooth curve over all rows ---
    # Build an array of size H with NaNs and then interpolate
    curve = np.full((h,), np.nan, dtype=np.float32)
    curve[row_indices] = row_centers

    # Interpolate NaNs linearly between valid rows
    valid = ~np.isnan(curve)
    ys_valid = np.where(valid)[0]
    xs_valid = curve[valid]

    ys_all = np.arange(h)
    curve_interp = np.interp(ys_all, ys_valid, xs_valid)

    # Optional smoothing along rows
    if gaussian_smooth_rows > 1:
        ksize = gaussian_smooth_rows if gaussian_smooth_rows % 2 == 1 else gaussian_smooth_rows + 1
        curve_smooth = cv2.GaussianBlur(curve_interp.reshape(-1, 1), (1, ksize), 0).reshape(-1)
    else:
        curve_smooth = curve_interp

    # --- 4) Apply row-wise cut along curve ---
    drivable = road_bool.copy()
    before = np.sum(drivable)

    for y in range(h):
        x_cut = int(curve_smooth[y])
        x_cut = max(0, min(w, x_cut))
        drivable[y, :x_cut] = False

    # after = np.sum(drivable)
    # print("Row-wise curve cut removed pixels:", before - after)

    return drivable





def run_and_save_mask2former_overlay(img_path: Path):
    # Load image
    image_pil = Image.open(img_path).convert("RGB")
    h, w = image_pil.height, image_pil.width
    image_np = np.array(image_pil)  # H x W x 3, RGB

    # Run Mask2Former
    inputs = image_processor(images=image_pil, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)

    semseg = image_processor.post_process_semantic_segmentation(
        outputs, target_sizes=[(h, w)]
    )[0].cpu().numpy()  # H x W

    # === IMPORTANT: use same road definition as eval loop ===
    # Eval loop used is_pred_road[semseg]
    pred_road = is_pred_road[semseg]

    pred_drivable = refine_drivable_keep_right_of_centerline_debug(image_np,pred_road)

    # Optional: print debug stats
    print("pred_road sum:", np.sum(pred_road))
    print("pred_drivable sum:", np.sum(pred_drivable))
    print("pixels removed by refine:", np.sum(pred_road & ~pred_drivable))

    # ------------------------
    # Make shaded overlay using DRIVABLE mask
    # ------------------------
    overlay = image_np.copy()
    alpha = 0.4  # transparency for overlay
    color = np.array([0, 255, 0], dtype=np.uint8)  # green for drivable

    mask = pred_drivable  # H x W bool
    overlay[mask] = (alpha * color + (1.0 - alpha) * overlay[mask]).astype(np.uint8)

    stem = img_path.stem
    suffix = img_path.suffix
    out_name = f"{stem}_mask2former_inference{suffix}"
    out_path = PRED_SAVE_DIR / out_name


    Image.fromarray(overlay).save(out_path)
    print(f"Saved overlay: {out_path}")
    return overlay

# ------------------------
# Run on selected filenames
# ------------------------
def get_qualitative(filenames: list[str]):
    
    for fname in tqdm(filenames, desc="Running Mask2Former"):
        img_path = EVAL_IMG_PATH / fname
        if not img_path.exists():
            print(f"[WARN] Image not found: {img_path}")
            continue
        overlay_img = run_and_save_mask2former_overlay(img_path)


        plt.figure(figsize=(14,7))

        plt.subplot(1,2,1)
        plt.imshow(np.array(Image.open(img_path).convert("RGB")))
        plt.axis('off')
        plt.title("Original")

        plt.subplot(1,2,2)
        plt.imshow(overlay_img)
        plt.axis('off')
        plt.title("Refined Drivable-Area Overlay")

        plt.show()
